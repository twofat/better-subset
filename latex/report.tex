\documentclass[12pt]{article}
\usepackage{amsmath,bbm,amssymb}
\usepackage{amsthm}
\usepackage{comment}
\usepackage{graphicx, graphics,floatrow}
\usepackage{textcomp}
\usepackage{amsfonts}
\usepackage[lined,boxed,commentsnumbered]{algorithm2e}
\usepackage{psfrag,pst-node,subfigure,rotating, amsmath, bbm, amsthm, amssymb, amsthm, setspace, picture, epsfig, amsfonts, upgreek}

%%% Added on 04/13/2009 for getting multiple rows in Table 4.
\usepackage{multirow}


%%% Dashed vertical and horizontal lines
\usepackage{arydshln}
\theoremstyle{definition}
\newtheorem{assumption}{Assumption}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem*{lemma}{Lemma}
\newtheorem{remark}{Remark}
%\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}
\newtheorem{definition}[remark]{Definition}


\textwidth=6.5in \textheight=8.6in \oddsidemargin=0.0in \evensidemargin=0.0in \topmargin=-0.4in
\renewcommand{\baselinestretch}{1.5}
%\newenvironment{example}[1][Example]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newcommand{\bm}[1]{\mbox{\boldmath$ #1 $\unboldmath}}

%\numberwithin{equation}{section}
%\numberwithin{figure}{section}

%%% Defined on March 17, 2009

\def\v{\mathbf}\def\m{\mathbf}\def\vg{\boldsymbol}
\def\med{{\mathrm{med}}} \def\MAD{{\mathrm{MAD}}} \def\MADN{{\mathrm{MADN}}}
\def\P{{\mathrm{F}}} \def\E{{\mathrm{E}}} \def\V{{\mathrm{var}}}

%%% Added on 04/06, 2009
%%\newcommand{\bm}[1]{\mbox{\boldmath$ #1 $\unboldmath}}


\title{\bf Report About Improved Screening in High Dimensional Regression}

\date{ }
\begin{document}

\maketitle \noindent{ABSTRACT:} {\normalsize \ \ \ \
Inspired by Xiong \cite{xiong} and \cite{mce}, we arrange a algorithm to find M variables with small RSS(residual sum of square) in regression model, in which the M is fixed. According to the asymptotic criterion that better fitting means better submodel supposed by Xiong \cite{xiong},
our improved method is useful in some cases. Due to the high dimensionality, our algorithm can not find the submodel with the least RSS. But we can reduce the RSS based on the other methods such as LAR, FS or SIS.

}

\vspace{8mm}
 \noindent{\normalsize {\bf Keywords}: \ \ \ \ Screening, Branch and Bound Algorithm, EM Algorithm, Orthogonalizing Subset Screening}

%\newpage

\section{\bf Introduction}
\hskip\parindent
\vspace{-0.8cm}
\subsection{Branch and Bound Algorithm}
See \cite{mce} for detail...\\
BBA actually gives a method to search all the combination of any M-variables. In some degree, it construct a tree of models and search it using Breadth-First-Search. The standard BBA is obviously computational infeasible in relatively high dimensions. A pre-ordering BBA is considered to find the subset with smaller RSS efficiently, which is adopted in our algorithm.

\subsection{Orthogonalizing Subset Screening}
See \cite{xiong} for detail...\\
It must be noted that there is an $\beta^{(ini)}$ and a $\beta^{(result)}$ in each iterative in this algorithm. According to xiong, $\beta^{(result)}$ behaves better than the $\beta^{(ini)}$.\\
On the other hand, if X-matrix is standardized, we can regard the each absolute value of each $\beta$ in the model as a order to measure the importance of each variable in the model. \\
Since $\beta_0 = \beta^{(ini)}$, according to \cite{xiong},
\begin{equation}
\beta_{k+1} = S_M\{a^{-1}X^Ty + (I-a^{-1}X^TX)\beta^{(k)}\}
\end{equation}
in our article, to give every variable a unique order, for the $k_{max}$(largest k) in the iterative:
\begin{equation}
\beta^{(result)} = a^{-1}X^Ty + (I-a^{-1}X^TX)\beta^{(k_{max}-1)}
\end{equation}
Hence $$\beta^{(k_{max})}=S_M\{\beta^{(result)}\}$$.


\section{\bf Our method}
\hskip\parindent
\vspace{-0.8cm}
We prompt a combination of the two methods above. \\\\ When we have a $\beta^{(ini)}$, similar to the setting of node in \cite{mce}, each node
consist (S, k, $\beta$), where $|\beta|=|S|$. $\beta$ is the estimates of coefficients in S computed by OSS(Orthogonalizing Subset Screening), which is considered to be the base of sorting.

\IncMargin{1em}
\begin{algorithm}
\SetAlgoLined
\caption{Method}
\BlankLine
flag = 0\;
n = $|V|$\;
Insert (V, 0, $\beta^{(ini)}$) into node list\;
 \While{$(n_s>M) \& (flag<maximum)$}{
 flag = flag + 1\;
 Extract (S, k, $\beta$) from node list\;
 Update $\beta$ by OSS under variable set S with N iterations\;
  $n_s$ = $|S|$\;
  Resort S by abs of$\beta$\;
  compute residuals of $[s_1,...,s_M]$ and update the min-residuals\;
  \For{$i\leftarrow (k+1)$ \KwTo $\min(M,n_s-1)$}{
    $S' \leftarrow drop(S, i)$\;
    $\beta' \leftarrow$ drop($\beta$, i)\;
    Insert (S', i-1, $\beta$') into node list\;
  }
}
\end{algorithm}\DecMargin{1em}
\par In which the maximum and the N is pre-given.
\newpage

\section{Some strategies}
\hskip\parindent
\vspace{-0.8cm}\newline
Due to the searching-process in our method, our method is relatively computational intensive compared to the others(such as the iterative OSS).
To mitigate this problem in some degree, we adopt some strategies as following:
\subsection{Radius pre-ordering}
Similar to the pre-ordering BBA, we could try to define radius for each node. The root nodes of a larger tree owns the smaller radius.
The update of $\beta$ using OSS algorithm is the corresponding procedure of pre-ordering in our case.\\

\IncMargin{1em}
\begin{algorithm}
\SetAlgoLined
\caption{Radius-Method}
\BlankLine
flag = 0\;
n = $|V|$\;
Insert (V, 0, $\beta^{(ini)}$) into node list\;
 \While{$(n_s>M) \& (flag<maximum)$}{
 flag = flag + 1\;
 Extract (S, k, $\beta$) from node list\;
  $n_s$ = $|S|$\;
  $\rho$ = k+n-$n_s$\;
 \If{$\rho<P$}{
    Update $\beta$ by OSS under variable set S with N iterations\;
    Resort S by abs of $\beta$\;
   }
  compute residuals of $[s_1,...,s_M]$ and update the min-residuals\;
  \For{$i\leftarrow (k+1)$ \KwTo $\min(M,n_s-1)$}{
    $S' \leftarrow drop(S, i)$\;
    $\beta' \leftarrow$ drop($\beta$, i)\;
    Insert (S', i-1, $\beta$') into node list\;
  }
}
\end{algorithm}\DecMargin{1em}
In which, P is the threshold radius fixed in advance.
\newpage
\subsection{Heuristic strategies}
To be more efficient, we attempt to use some heuristic strategies. In this section, simulated annealing are used to drop some modes with much higher RSS compared to their father nodes. Since each node is generated by dropping a variable from his father node, if a node with higher RSS than his father node, it is much probably that the dropped variable is important. So we will cast this node at the probability of $P(RSS_{current}, RSS_{father}, T)$ if $RSS_{current} > RSS_{father}$.
where
$$P(A, B, T) = 1-\exp\{-(A-B)/T\}$$.

\IncMargin{1em}
\begin{algorithm}
\SetAlgoLined
\caption{Heuristic Radius-Method}
\BlankLine
flag = 0\;
n = $|V|$\;
$RSS_{father}$ = $\infty$;
Insert (V, 0, $\beta^{(ini)}$, $RSS_{father}$) into node list\;
 \While{$(n_s>M) \& (flag<maximum)$}{
 flag = flag + 1\;
 Extract (S, k, $\beta$, $RSS_{father}$) from node list\;
  $n_s$ = $|S|$\;
  $\rho$ = k+n-$n_s$\;
 \If{$\rho<P$}{
    Update $\beta$ by OSS under variable set S with N iterations\;
    Resort S by abs of $\beta$\;
   }
  $RSS_{current}$ = residuals of $[s_1,...,s_M]$\;
   update the min-residuals\;
   \If{$P(RSS_{current}, RSS_{father}, T)>random()$}{
        continue\;
   }
   $RSS_{father}=RSS_{current}$\;
    \For{$i\leftarrow (k+1)$ \KwTo $\min(M,n_s-1)$}{
    $S' \leftarrow drop(S, i)$\;
    $\beta' \leftarrow$ drop($\beta$, i)\;
    Insert (S', i-1, $\beta$',$RSS_{father}$) into node list\;
  }
}
\end{algorithm}\DecMargin{1em}


\subsection{Cutoff some variables}
In some high dimensional situations, searching the better subset using our method are remains slowly. Intuitively, true variables are tent to rank relatively in front even though some of them are not in top-M (I am not sure about whether there is such theory or not...). So we try to cast $Q$ least-important variables in each node. We set Q equals to $(n_s-M-1)/L$, in which $L$ is a tuning parameter used for control.


\section{\bf Simulations}
\hskip\parindent
\vspace{-0.8cm}
Some of the simulation result:\\

\begin{table}[ht]
\ttabbox{\caption{n=50, p=100, sd=1}}{
\centering
\begin{minipage}{0.5\textwidth}
\subtable[Coverage Rate(\%)]{
\begin{tabular}{rrrr}
  \hline
 & ini & EM-Im & HRBBA \\
  \hline
SIS & 71.20 & 89.90 & 93.60 \\
  SF & 83.70 & 83.70 & 83.70 \\
  Lar & 96.00 & 96.60 & 96.50 \\
  LASSO & 95.00 & 96.80 & 97.00 \\
  Elastic & 95.80 & 96.70 & 96.80 \\
  GLM & 88.20 & 90.80 & 93.00 \\
  SCAD & 90.10 & 91.50 & 92.70 \\
   \hline
\end{tabular}
}
\end{minipage}
\hfil
\begin{minipage}{0.5\textwidth}
\subtable[Residual Sum of Square]{
\begin{tabular}{rrrr}
  \hline
 & ini & EM-Im & HRBBA \\
  \hline
SIS & 296.83 & 23.95 & 11.70 \\
  SF & 14.85 & 14.52 & 10.93 \\
  Lar & 48.45 & 19.57 & 10.73 \\
  LASSO & 51.66 & 18.56 & 10.24 \\
  Elastic & 44.84 & 18.57 & 10.16 \\
  GLM & 93.49 & 22.82 & 11.57 \\
  SCAD & 43.32 & 17.69 & 10.57 \\
   \hline
\end{tabular}
}
\end{minipage}}
\end{table}

\begin{table}[ht]
\ttabbox{\caption{n=100, p=300, sd=1.5}}{
\centering
\begin{minipage}{0.5\textwidth}
\subtable[Coverage Rate(\%)]{
\begin{tabular}{rrrr}
  \hline
 & ini & EM-Impr & H-Radius \\
  \hline
SIS & 77.20 & 91.90 & 95.70 \\
  SF & 97.50 & 97.50 & 97.40 \\
  Lar & 97.80 & 98.10 & 99.30 \\
  LASSO & 98.20 & 98.70 & 99.10 \\
  Elastic & 98.70 & 98.40 & 99.30 \\
  GLM & 87.20 & 92.50 & 96.90 \\
  SCAD & 99.10 & 99.10 & 99.10 \\
   \hline
\end{tabular}
}
\end{minipage}
\hfil
\begin{minipage}{0.5\textwidth}
\subtable[Residual Sum of Square]{
\begin{tabular}{rrrr}
  \hline
 & ini & EM-Impr & H-Radius \\
  \hline
SIS & 851.78 & 144.92 & 103.46 \\
  SF & 92.57 & 92.47 & 87.72 \\
  Lar & 316.41 & 133.64 & 98.98 \\
  LASSO & 227.49 & 127.14 & 98.22 \\
  Elastic & 223.97 & 128.50 & 99.09 \\
  GLM & 450.35 & 152.48 & 104.07 \\
  SCAD & 136.26 & 103.73 & 90.65 \\
   \hline
\end{tabular}
}
\end{minipage}}
\end{table}

\vspace{1cm} \noindent{\bf Acknowledgements}

%\vspace{1cm}
\bibliographystyle{model1a-num-names}
\bibliography{<your-bib-database>}
 \begin{thebibliography}{10}
\bibitem{mce}
Hofmann, Marc, Cristian Gatu, and Erricos John Kontoghiorghes. "Efficient algorithms for computing the best subset regression models for large-scale problems." \textit{Computational Statistics \& Data Analysis} 52.1 (2007): 16-29.

\bibitem{xiong}
Xiong, Shifeng. "Better subset regression." Biometrika (2013): ast041.


 \end{thebibliography}
\end{document}



